{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2 as cv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import idx2numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "##reading_images\n",
    "file = 'faishon_mnist/train-images-idx3-ubyte'\n",
    "training_data = idx2numpy.convert_from_file(file)\n",
    "\n",
    "\n",
    "file=\"faishon_mnist/t10k-images-idx3-ubyte\"\n",
    "test_data=idx2numpy.convert_from_file(file)\n",
    "\n",
    "##reading labels\n",
    "\n",
    "file = 'faishon_mnist/train-labels-idx1-ubyte'\n",
    "training_labels = idx2numpy.convert_from_file(file)\n",
    "\n",
    "\n",
    "file=\"faishon_mnist/t10k-labels-idx1-ubyte\"\n",
    "test_labels=idx2numpy.convert_from_file(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=np.expand_dims(training_data,axis=1)\n",
    "test_data=np.expand_dims(test_data,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28, 1) (60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "training_data=training_data.reshape(60000,28,28,1)\n",
    "test_data=test_data.reshape(10000,28,28,1)\n",
    "print(test_data.shape,training_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,) (60000,)\n"
     ]
    }
   ],
   "source": [
    "print(test_labels.shape,training_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#plt.imshow(training_data[4][1:])\n",
    "print(training_labels[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels={0: \"T-shirt/top\",\n",
    "1: \"Trouser\",\n",
    "2: \"Pullover\",\n",
    "3: \"Dress\",\n",
    "4: \"Coat\",\n",
    "5: \"Sandal\",\n",
    "6: \"Shirt\",\n",
    "7: \"Sneaker\",\n",
    "8: \"Bag\",\n",
    "9: \"Ankle boot\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img,img_to_array\n",
    "from tensorflow.keras.models import Sequential,Model,load_model\n",
    "from tensorflow.keras.layers import Dense,Conv2D,Flatten,MaxPooling2D,Dropout\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.layers import Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "model=Sequential()\n",
    "model.add(Conv2D(32,kernel_size=3,input_shape=(28,28,1),activation='relu'))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Conv2D(32,kernel_size=3,activation='relu'))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 57s 951us/sample - loss: 0.2988 - accuracy: 0.8895 - val_loss: 0.3447 - val_accuracy: 0.8774\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 56s 933us/sample - loss: 0.2937 - accuracy: 0.8904 - val_loss: 0.3279 - val_accuracy: 0.8862\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 56s 941us/sample - loss: 0.2908 - accuracy: 0.8916 - val_loss: 0.3364 - val_accuracy: 0.8872\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 56s 939us/sample - loss: 0.2857 - accuracy: 0.8932 - val_loss: 0.3169 - val_accuracy: 0.8877\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 57s 945us/sample - loss: 0.2797 - accuracy: 0.8956 - val_loss: 0.3259 - val_accuracy: 0.8881\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 57s 946us/sample - loss: 0.2843 - accuracy: 0.8937 - val_loss: 0.3264 - val_accuracy: 0.8877\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 57s 952us/sample - loss: 0.2784 - accuracy: 0.8971 - val_loss: 0.3478 - val_accuracy: 0.8871\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 57s 950us/sample - loss: 0.2743 - accuracy: 0.8971 - val_loss: 0.3443 - val_accuracy: 0.8866\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 57s 949us/sample - loss: 0.2738 - accuracy: 0.8982 - val_loss: 0.3318 - val_accuracy: 0.8919\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 57s 942us/sample - loss: 0.2691 - accuracy: 0.9005 - val_loss: 0.3167 - val_accuracy: 0.8921\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 56s 941us/sample - loss: 0.2667 - accuracy: 0.9005 - val_loss: 0.3551 - val_accuracy: 0.8841\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 56s 940us/sample - loss: 0.2674 - accuracy: 0.8995 - val_loss: 0.3624 - val_accuracy: 0.8865\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 57s 942us/sample - loss: 0.2608 - accuracy: 0.9020 - val_loss: 0.3420 - val_accuracy: 0.8876\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 57s 942us/sample - loss: 0.2630 - accuracy: 0.9019 - val_loss: 0.3379 - val_accuracy: 0.8891\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 57s 944us/sample - loss: 0.2559 - accuracy: 0.9045 - val_loss: 0.3795 - val_accuracy: 0.8894\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 57s 945us/sample - loss: 0.2551 - accuracy: 0.9034 - val_loss: 0.3483 - val_accuracy: 0.8943\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 57s 947us/sample - loss: 0.2548 - accuracy: 0.9023 - val_loss: 0.3357 - val_accuracy: 0.8935\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 57s 944us/sample - loss: 0.2591 - accuracy: 0.9035 - val_loss: 0.3521 - val_accuracy: 0.8916\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 57s 948us/sample - loss: 0.2513 - accuracy: 0.9049 - val_loss: 0.3571 - val_accuracy: 0.8962\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 57s 942us/sample - loss: 0.2581 - accuracy: 0.9034 - val_loss: 0.3562 - val_accuracy: 0.8812\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 61s 1ms/sample - loss: 0.2533 - accuracy: 0.9043 - val_loss: 0.3627 - val_accuracy: 0.8872\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 58s 960us/sample - loss: 0.2551 - accuracy: 0.9040 - val_loss: 0.3544 - val_accuracy: 0.8895\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 58s 974us/sample - loss: 0.2450 - accuracy: 0.9070 - val_loss: 0.3561 - val_accuracy: 0.8903\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 58s 969us/sample - loss: 0.2503 - accuracy: 0.9068 - val_loss: 0.3945 - val_accuracy: 0.8924\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 59s 978us/sample - loss: 0.2447 - accuracy: 0.9085 - val_loss: 0.4132 - val_accuracy: 0.8931\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 58s 968us/sample - loss: 0.2512 - accuracy: 0.9058 - val_loss: 0.3811 - val_accuracy: 0.8936\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 58s 965us/sample - loss: 0.2422 - accuracy: 0.9082 - val_loss: 0.3703 - val_accuracy: 0.8942\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 60s 1ms/sample - loss: 0.2474 - accuracy: 0.9062 - val_loss: 0.3846 - val_accuracy: 0.8908\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 60s 997us/sample - loss: 0.2414 - accuracy: 0.9082 - val_loss: 0.4079 - val_accuracy: 0.8881\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 61s 1ms/sample - loss: 0.2470 - accuracy: 0.9092 - val_loss: 0.3842 - val_accuracy: 0.8902\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 58s 974us/sample - loss: 0.2470 - accuracy: 0.9080 - val_loss: 0.3848 - val_accuracy: 0.8962\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 58s 974us/sample - loss: 0.2419 - accuracy: 0.9075 - val_loss: 0.4033 - val_accuracy: 0.8904\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 59s 982us/sample - loss: 0.2427 - accuracy: 0.9082 - val_loss: 0.3958 - val_accuracy: 0.8931\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 58s 974us/sample - loss: 0.2517 - accuracy: 0.9072 - val_loss: 0.3951 - val_accuracy: 0.8749\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 58s 965us/sample - loss: 0.2381 - accuracy: 0.9097 - val_loss: 0.4145 - val_accuracy: 0.8942\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 59s 983us/sample - loss: 0.2375 - accuracy: 0.9105 - val_loss: 0.4270 - val_accuracy: 0.8874\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 59s 979us/sample - loss: 0.2398 - accuracy: 0.9091 - val_loss: 0.3919 - val_accuracy: 0.8901\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 60s 1ms/sample - loss: 0.2407 - accuracy: 0.9099 - val_loss: 0.4132 - val_accuracy: 0.8896\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 61s 1ms/sample - loss: 0.2375 - accuracy: 0.9109 - val_loss: 0.4117 - val_accuracy: 0.8934\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 60s 1ms/sample - loss: 0.2342 - accuracy: 0.9113 - val_loss: 0.4238 - val_accuracy: 0.8915\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 60s 1ms/sample - loss: 0.2428 - accuracy: 0.9098 - val_loss: 0.4085 - val_accuracy: 0.8927\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 60s 993us/sample - loss: 0.2304 - accuracy: 0.9140 - val_loss: 0.4002 - val_accuracy: 0.8930\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 60s 995us/sample - loss: 0.2387 - accuracy: 0.9108 - val_loss: 0.4057 - val_accuracy: 0.8902\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 59s 991us/sample - loss: 0.2342 - accuracy: 0.9112 - val_loss: 0.4075 - val_accuracy: 0.8882\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 59s 981us/sample - loss: 0.2284 - accuracy: 0.9135 - val_loss: 0.4272 - val_accuracy: 0.8965\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 60s 998us/sample - loss: 0.2443 - accuracy: 0.9099 - val_loss: 0.4123 - val_accuracy: 0.8895\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 59s 982us/sample - loss: 0.2325 - accuracy: 0.9121 - val_loss: 0.4247 - val_accuracy: 0.8864\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 59s 986us/sample - loss: 0.2413 - accuracy: 0.9123 - val_loss: 0.4073 - val_accuracy: 0.8869\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 59s 978us/sample - loss: 0.2298 - accuracy: 0.9133 - val_loss: 0.4318 - val_accuracy: 0.8814\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 59s 991us/sample - loss: 0.2323 - accuracy: 0.9134 - val_loss: 0.4407 - val_accuracy: 0.8898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a682f0ebe0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(training_data,training_labels,validation_data=(test_data,test_labels),epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"faishon_mnist/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 271us/sample - loss: 0.4407 - accuracy: 0.8898\n",
      "The accuracy of model in test data is: 88.98 %\n",
      "The loss of model in test data is :  0.441\n"
     ]
    }
   ],
   "source": [
    "score=model.evaluate(test_data,test_labels)\n",
    "print(\"The accuracy of model in test data is: {} %\".format(round(score[1]*100,3)))\n",
    "print(\"The loss of model in test data is :  {}\".format(round(score[0],3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore(image,label):\n",
    "    image=image.reshape(28,28,1)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(image)\n",
    "    print(\"#################\",labels[label],\"########\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################# Sandal ########\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "k=11\n",
    "explore(test_data[k],test_labels[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "##automating the classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As in case of faishon nist dataset the images are already given in the form of numpy array but in thr real word we have the images in the folder so, this code is used for different datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing necessary Libraries \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL \n",
    "from PIL import Image\n",
    "import cv2 as cv\n",
    "import os\n",
    "\n",
    "\n",
    "## importing tenserflow libraries for making deeplearning Model\n",
    "from keras.preprocessing.image import ImageDataGenerator,load_img,img_to_array\n",
    "from keras.layers import Dense,Flatten,Conv2D,MaxPooling2D,Dropout\n",
    "from keras.models import Model,Sequential\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Activation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the data should be in different folder train_data_folder and test_data_folder and in these folder we must have subfolder containing images of different categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder=\"train_folder_location\"\n",
    "test_folder=\"test_folder_location\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen=ImageDataGenerator(rescale=1/255,horizontal_flip=True,shear_range=.2,zoom_range=.2)\n",
    "test_datagen=ImageDataGenerator(rescale=1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator=train_datagen.flow_from_directory(train_folder,target_size=(224,224),class_mode='categorical',shuffle=True,batch_size=25)\n",
    "test_generator=train_datagen.flow_from_directory(test_folder,target_size=(224,224),class_mode='categorical',shuffle=True,batch_size=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=train_generator.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we have large dataset in real word so we are using Transfer learning instead of making our new model from scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_classes=len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transfer learning\n",
    "vgg=VGG16(input_shape=(224,224,3),weights='imagenet',include_top=False)\n",
    "for layer in vgg.layers:\n",
    "    layer.trainable=False\n",
    "x=Flatten()(vgg.output)\n",
    "x=Dense(50,activation='relu')(x)\n",
    "final=Dense(no_of_classes,activation='softmax')(x)\n",
    "model1=Model(inputs=vgg.input,outputs=final)\n",
    "model1.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fit_generator(train_generator,epochs=25,steps_per_epoch=len(train_generator),validation_generator=test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save(\"faishon_mnist/model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
